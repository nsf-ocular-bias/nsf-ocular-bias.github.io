[{"authors":null,"categories":null,"content":"Sreeraj Ramachandran is a PhD scholar at the Vision Computing and Biometrics Security Lab, Wichita State University. His research include Computer Vision, Biometrics, Bias AI, GANs and adversarial Attacks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/sreeraj-ramachandran/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sreeraj-ramachandran/","section":"authors","summary":"Sreeraj Ramachandran is a PhD scholar at the Vision Computing and Biometrics Security Lab, Wichita State University. His research include Computer Vision, Biometrics, Bias AI, GANs and adversarial Attacks.","tags":null,"title":"Sreeraj Ramachandran","type":"authors"},{"authors":["Anoop Krishnan"],"categories":null,"content":"Anoop Krishnan is a Ph.D. student at Vision Computing and Biometric Security Lab,Wichita State University. He does research on Fair and Ethical Biometric Systems, which involves studying about the biases in the present systems and mitigate it through deep learning techniques.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da80271efc1c2a67d689eedf34452e4d","permalink":"/author/anoop-krishnan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/anoop-krishnan/","section":"authors","summary":"Anoop Krishnan is a Ph.D. student at Vision Computing and Biometric Security Lab,Wichita State University. He does research on Fair and Ethical Biometric Systems, which involves studying about the biases in the present systems and mitigate it through deep learning techniques.","tags":null,"title":"Anoop Krishnan","type":"authors"},{"authors":["Ajita Rattani"],"categories":null,"content":"Ajita Rattani is an Assistant Professor in the Department of Electrical Engineering and Computer Science at Wichita State University. Prior to this, she was an Adjunct Graduate Faculty at the University of Missouri - Kansas City. She did her Post-doctoral studies from the Department of Computer Science and Engineering, Michigan State University, USA. Ajita obtained her Ph.D. in Computer Science Engineering from the University of Cagliari, Italy.\nHer research interest includes Pattern Recognition, Classifier Fusion, Machine/ Deep Learning, Image Processing, Computer Vision, and Biometrics. She has bagged more than 100 research papers in high impact international conferences and journals. She is the co-editor of the Springer books titled “Adaptive Biometric Systems: Recent Advances and Challenges” and \u0026ldquo;Selfie Biometrics: Advances and Challenges\u0026rdquo;.\nAjita has been the recipient of the Best Paper and Poster awards at IEEE IJCB 2014, IEEE HST 2017 and IAPR Biometric Summer School 2008. She has collaborated with many eminent scientists across the globe in her field of research. She has also spent one year in University of Sassari, Italy (2006-2007) and six months in West Virginia University, USA (2009) as a Visiting Researcher.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6aad9adc76eb3531441fe24660f519b5","permalink":"/author/dr.-ajita-rattani/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dr.-ajita-rattani/","section":"authors","summary":"Ajita Rattani is an Assistant Professor in the Department of Electrical Engineering and Computer Science at Wichita State University. Prior to this, she was an Adjunct Graduate Faculty at the University of Missouri - Kansas City.","tags":null,"title":"Dr. Ajita Rattani","type":"authors"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"/event/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/example/","section":"event","summary":"An example event.","tags":[],"title":"Example Event","type":"event"},{"authors":null,"categories":[],"content":"Dataset Gender Classification Results              Subject Verification Results        Subject Verification All Results Table  Visualizations GradCAM GradCAM         Guided GradCAM Guided GradCAM         Occlusion Sensitivity Occlusion Sensitivity         Vanilla Gradients Vanilla Gradients         Integrated Gradients Integrated Gradients         SmoothGrad         -- Gradients x Input Gradients x Input         Averaged Map \u0026times;       References Bibliography called, but no references ","date":1635698152,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635698152,"objectID":"a13db798a5b55a04223e32f31d7512c3","permalink":"/updates/notredame-results/","publishdate":"2021-10-31T11:35:52-05:00","relpermalink":"/updates/notredame-results/","section":"updates","summary":"Dataset Gender Classification Results              Subject Verification Results        Subject Verification All Results Table  Visualizations GradCAM GradCAM         Guided GradCAM Guided GradCAM         Occlusion Sensitivity Occlusion Sensitivity         Vanilla Gradients Vanilla Gradients         Integrated Gradients Integrated Gradients         SmoothGrad         -- Gradients x Input Gradients x Input         Averaged Map \u0026times;       References Bibliography called, but no references ","tags":[],"title":"Notredame Preliminary Results","type":"updates"},{"authors":null,"categories":[],"content":"Dataset The UFPR-Periocular dataset was created to obtain images in unconstrained scenarios that contain realistic noises caused by occlusion, blur, and variations in lighting, distance, and angles.\nThe gender distribution of the subjects is $(53,65\\%)$ male and $(46,35\\%)$ female, and approximately $66\\%$ of the subjects are under $31$ years old. In total, the dataset has images captured from $196$ different mobile devices – the five most used device models were: Apple iPhone 8 $(4.1\\%)$, Apple iPhone 9 $(3.1\\%)$, Xiaomi Mi 8 Lite $(3.0\\%)$, Apple iPhone 7 $(3.0\\%)$, and Samsung Galaxy J7 Prime $(2.7\\%)$.\nGender Classification Results              Subject Verification Results        Subject Verification All Results Table  Visualizations GradCAM GradCAM         Guided GradCAM Guided GradCAM         Occlusion Sensitivity Occlusion Sensitivity         Vanilla Gradients Vanilla Gradients         Integrated Gradients Integrated Gradients         SmoothGrad SmoothGrad         Gradients x Input Gradients x Input         Averaged Map \u0026times;       References Bibliography called, but no references ","date":1635698152,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635698152,"objectID":"c155b7f4ec734e028eaee7a3857a0979","permalink":"/updates/ufpr-results/","publishdate":"2021-10-31T11:35:52-05:00","relpermalink":"/updates/ufpr-results/","section":"updates","summary":"Dataset The UFPR-Periocular dataset was created to obtain images in unconstrained scenarios that contain realistic noises caused by occlusion, blur, and variations in lighting, distance, and angles.\nThe gender distribution of the subjects is $(53,65\\%)$ male and $(46,35\\%)$ female, and approximately $66\\%$ of the subjects are under $31$ years old.","tags":[],"title":"UFPR Preliminary Results","type":"updates"},{"authors":["Anoop Krishnan","Ali Almadan","Ajita Rattani"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1634083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634083200,"objectID":"99f1735efaaff9ce390130a4b2a0e8bf","permalink":"/publication/iccst_2021/","publishdate":"2021-10-13T00:00:00Z","relpermalink":"/publication/iccst_2021/","section":"publication","summary":"A number of studies suggest bias of the face biometrics, i.e., face recognition and soft-biometric estimation methods, across gender, race, and age-groups. There is a recent urge to investigate the bias of different biometric modalities toward the deployment of fair and trustworthy biometric solutions. Ocular biometrics has obtained increased attention from academia and industry due to its high accuracy, security, privacy, and ease of use in mobile devices. A recent study in $2020$ also suggested the fairness of ocular-based user recognition across males and females. This paper aims to evaluate the fairness of ocular biometrics in the visible spectrum among age-groups; young, middle, and older adults. Thanks to the availability of the latest large-scale $2020$ UFPR ocular biometric dataset, with subjects acquired in the age range $18$ - $79$ years, to facilitate this study. Experimental results suggest the overall equivalent performance of ocular biometrics across gender and age-groups in user verification and gender-classification. Performance difference for older adults at lower false match rate and young adults was noted at user verification and age-classification, respectively. This could be attributed to inherent characteristics of the biometric data from these age-groups impacting specific applications, which suggest a need for advancement in sensor technology and software solutions.","tags":["Fairness","Bias","Ocular","Analysis"],"title":"Investigating Fairness of Ocular Biometrics Among Young, Middle-Aged, and Older Adults","type":"publication"},{"authors":null,"categories":null,"content":"Congratulations to Jian Yang and Monica Hall for winning the Best Paper Award at the 2020 Conference on Wowchemy for their paper “Learning Wowchemy”.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Integer tempus augue non tempor egestas. Proin nisl nunc, dignissim in accumsan dapibus, auctor ullamcorper neque. Quisque at elit felis. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Aenean eget elementum odio. Cras interdum eget risus sit amet aliquet. In volutpat, nisl ut fringilla dignissim, arcu nisl suscipit ante, at accumsan sapien nisl eu eros.\nSed eu dui nec ligula bibendum dapibus. Nullam imperdiet auctor tortor, vel cursus mauris malesuada non. Quisque ultrices euismod dapibus. Aenean sed gravida risus. Sed nisi tortor, vulputate nec quam non, placerat porta nisl. Nunc varius lobortis urna, condimentum facilisis ipsum molestie eu. Ut molestie eleifend ligula sed dignissim. Duis ut tellus turpis. Praesent tincidunt, nunc sed congue malesuada, mauris enim maximus massa, eget interdum turpis urna et ante. Morbi sem nisl, cursus quis mollis et, interdum luctus augue. Aliquam laoreet, leo et accumsan tincidunt, libero neque aliquet lectus, a ultricies lorem mi a orci.\nMauris dapibus sem vel magna convallis laoreet. Donec in venenatis urna, vitae sodales odio. Praesent tortor diam, varius non luctus nec, bibendum vel est. Quisque id sem enim. Maecenas at est leo. Vestibulum tristique pellentesque ex, blandit placerat nunc eleifend sit amet. Fusce eget lectus bibendum, accumsan mi quis, luctus sem. Etiam vitae nulla scelerisque, eleifend odio in, euismod quam. Etiam porta ullamcorper massa, vitae gravida turpis euismod quis. Mauris sodales sem ac ultrices viverra. In placerat ultrices sapien. Suspendisse eu arcu hendrerit, luctus tortor cursus, maximus dolor. Proin et velit et quam gravida dapibus. Donec blandit justo ut consequat tristique.\nAnd here is the citation of (Citation: Lessig) Lessig,\u0026#32; L. \u0026#32; (n.d.). \u0026#32;.  . Let me continue with the text and see what happens.\n References   Lessig (n.d.)  Lessig,\u0026#32; L. \u0026#32; (n.d.). \u0026#32;.     ","date":1606867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606867200,"objectID":"2a0ec8a990dbd78a00c4e15a09364b00","permalink":"/post/20-12-02-icml-best-paper/","publishdate":"2020-12-02T00:00:00Z","relpermalink":"/post/20-12-02-icml-best-paper/","section":"post","summary":"Congratulations to Jian Yang and Monica Hall for winning the Best Paper Award at the 2020 Conference on Wowchemy for their paper “Learning Wowchemy”.\n","tags":null,"title":"Jian Yang and Monica Hall Win the Best Paper Award at Wowchemy 2020","type":"post"},{"authors":null,"categories":null,"content":"Congratulations to Richard Hendricks for winning first place in the Wowchemy Prize.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Integer tempus augue non tempor egestas. Proin nisl nunc, dignissim in accumsan dapibus, auctor ullamcorper neque. Quisque at elit felis. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Aenean eget elementum odio. Cras interdum eget risus sit amet aliquet. In volutpat, nisl ut fringilla dignissim, arcu nisl suscipit ante, at accumsan sapien nisl eu eros.\nSed eu dui nec ligula bibendum dapibus. Nullam imperdiet auctor tortor, vel cursus mauris malesuada non. Quisque ultrices euismod dapibus. Aenean sed gravida risus. Sed nisi tortor, vulputate nec quam non, placerat porta nisl. Nunc varius lobortis urna, condimentum facilisis ipsum molestie eu. Ut molestie eleifend ligula sed dignissim. Duis ut tellus turpis. Praesent tincidunt, nunc sed congue malesuada, mauris enim maximus massa, eget interdum turpis urna et ante. Morbi sem nisl, cursus quis mollis et, interdum luctus augue. Aliquam laoreet, leo et accumsan tincidunt, libero neque aliquet lectus, a ultricies lorem mi a orci.\nMauris dapibus sem vel magna convallis laoreet. Donec in venenatis urna, vitae sodales odio. Praesent tortor diam, varius non luctus nec, bibendum vel est. Quisque id sem enim. Maecenas at est leo. Vestibulum tristique pellentesque ex, blandit placerat nunc eleifend sit amet. Fusce eget lectus bibendum, accumsan mi quis, luctus sem. Etiam vitae nulla scelerisque, eleifend odio in, euismod quam. Etiam porta ullamcorper massa, vitae gravida turpis euismod quis. Mauris sodales sem ac ultrices viverra. In placerat ultrices sapien. Suspendisse eu arcu hendrerit, luctus tortor cursus, maximus dolor. Proin et velit et quam gravida dapibus. Donec blandit justo ut consequat tristique.\n    Loss FaceSwap FaceSwap Face2Face Face2Face FaceShifter FaceShifter FaceSwap-K FaceSwap-K NeuralTextures NeuralTextures Deepfakes Deepfakes Celeb-DF Celeb-DF      Loss AUC EER AUC EER AUC EER AUC EER AUC EER AUC EER AUC EER   MS1M-AF Softmax 0.97 7.57 0.77 30.34 0.92 16.29 0.99 3.97 0.52 48.37 0.93 13.56 0.94 13.22   MS1M-AF ArcFace 0.99 4.23 0.75 33.03 0.96 9.50 0.99 2.40 0.50 49.53 0.94 10.70 0.96 9.07   MS1M-AF Combined 0.99 2.89 0.74 33.79 0.98 6.53 0.99 2.09 0.49 50.74 0.95 9.24 0.98 7.47   MS1M-AF SphereFace 0.98 4.61 0.80 27.61 0.96 10.99 0.98 3.50 0.52 48.72 0.95 10.67 0.95 10.63   MS1M-AF CosFace 0.99 2.68 0.72 35.55 0.98 5.15 0.99 2.04 0.49 51.10 0.96 8.38 0.98 7.10   MS1M-AF Triplet 0.91 14.49 0.64 39.99 0.93 13.66 0.96 8.10 0.47 52.04 0.87 20.75 0.79 28.04   WebFace12M Softmax 0.95 8.54 7.87 31.35 0.91 17.48 0.97 4.97 0.51 49.23 0.91 15.72 0.94 12.47   WebFace12M ArcFace 0.98 6.43 0.75 34.21 0.95 10.42 0.98 3.84 0.50 50.28 0.93 13.43 0.95 10.76   WebFace12M Combined 0.98 4.97 0.74 34.78 0.97 7.92 0.98 2.96 0.49 51.21 0.94 10.13 0.98 8.21   WebFace12M SphereFace 0.98 6.78 0.78 31.78 0.91 12.49 0.96 4.59 0.49 50.06 0.94 11.98 0.94 11.57   WebFace12M CosFace 0.98 4.98 0.72 37.89 0.97 6.83 0.97 3.07 0.48 52.45 0.95 9.48 0.97 9.23   WebFace12M Triplet 0.90 16.34 0.62 41.97 0.90 15.25 0.94 9.23 0.47 54.23 0.88 22.17 0.81 30.42   ","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"be2bd15f022f0d83fe9ffd743881e70c","permalink":"/post/20-12-01-wowchemy-prize/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/post/20-12-01-wowchemy-prize/","section":"post","summary":"Congratulations to Richard Hendricks for winning first place in the Wowchemy Prize.\n","tags":null,"title":"Richard Hendricks Wins First Place in the Wowchemy Prize","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]