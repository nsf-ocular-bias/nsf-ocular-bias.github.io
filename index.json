[{"authors":null,"categories":null,"content":"Sreeraj Ramachandran is a Graduate Research Assistant at the Vision Computing and Biometrics Security Lab, Wichita State University. His research include Computer Vision, Biometrics, Bias AI, GANs and adversarial Attacks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/sreeraj-ramachandran/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sreeraj-ramachandran/","section":"authors","summary":"Sreeraj Ramachandran is a Graduate Research Assistant at the Vision Computing and Biometrics Security Lab, Wichita State University. His research include Computer Vision, Biometrics, Bias AI, GANs and adversarial Attacks.","tags":null,"title":"Sreeraj Ramachandran","type":"authors"},{"authors":["Anoop Krishnan"],"categories":null,"content":"Anoop Krishnan is a Ph.D. student at Vision Computing and Biometric Security Lab,Wichita State University. He does research on Fair and Ethical Biometric Systems, which involves studying about the biases in the present systems and mitigate it through deep learning techniques.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da80271efc1c2a67d689eedf34452e4d","permalink":"/author/anoop-krishnan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/anoop-krishnan/","section":"authors","summary":"Anoop Krishnan is a Ph.D. student at Vision Computing and Biometric Security Lab,Wichita State University. He does research on Fair and Ethical Biometric Systems, which involves studying about the biases in the present systems and mitigate it through deep learning techniques.","tags":null,"title":"Anoop Krishnan","type":"authors"},{"authors":["Ajita Rattani"],"categories":null,"content":"Ajita Rattani is an Assistant Professor in the Department of Electrical Engineering and Computer Science at Wichita State University. Prior to this, she was an Adjunct Graduate Faculty at the University of Missouri - Kansas City. She did her Post-doctoral studies from the Department of Computer Science and Engineering, Michigan State University, USA. Ajita obtained her Ph.D. in Computer Science Engineering from the University of Cagliari, Italy.\nHer research interest includes Pattern Recognition, Classifier Fusion, Machine/ Deep Learning, Image Processing, Computer Vision, and Biometrics. She has bagged more than 100 research papers in high impact international conferences and journals. She is the co-editor of the Springer books titled “Adaptive Biometric Systems: Recent Advances and Challenges” and \u0026ldquo;Selfie Biometrics: Advances and Challenges\u0026rdquo;.\nAjita has been the recipient of the Best Paper and Poster awards at IEEE IJCB 2014, IEEE HST 2017 and IAPR Biometric Summer School 2008. She has collaborated with many eminent scientists across the globe in her field of research. She has also spent one year in University of Sassari, Italy (2006-2007) and six months in West Virginia University, USA (2009) as a Visiting Researcher.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6aad9adc76eb3531441fe24660f519b5","permalink":"/author/dr.-ajita-rattani/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dr.-ajita-rattani/","section":"authors","summary":"Ajita Rattani is an Assistant Professor in the Department of Electrical Engineering and Computer Science at Wichita State University. Prior to this, she was an Adjunct Graduate Faculty at the University of Missouri - Kansas City.","tags":null,"title":"Dr. Ajita Rattani","type":"authors"},{"authors":[],"categories":null,"content":"Goals and Technical Issues addressed  To address significant advances in the field of fairness and bias in AI with an application to biometrics systems. To provide a common forum for applied and academic researchers to exchange ideas on metrics and methods for evaluating and mitigating demographic effects in biometric performance. To address the disconnect between the work done by government, academicians, and industry.  Topics Covered  Statistical modeling of the demographic bias of the biometric systems across age, gender, and race in different image spectrums. Theoretical explanation and nomenclature regarding demographic effects in biometric systems. Modeling the compound impact of covariates such as lighting variations, make-up, pose, etc., in differential accuracy of the biometric systems across demographic variations. Explainable AI techniques to understand the cause of demographic variations in biometric systems. Evaluation of bias of speaker recognition system, and other behavioral biometrics. Biological origins of demographic effects on physical and behavioral biometric traits. Bias mitigation techniques that offer the best trade-off between accuracy and bias. Bias mitigation techniques in the absence of demographic variables. Techniques on enhancing privacy and fairness of the biometric surveillance system. Evaluation of bias of biometric data manipulation detection algorithms across demographics. Investigation of novel bias-free biometric modalities.  ","date":1661040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661040000,"objectID":"95c978aaad76ef6b30f8e44deaa67f12","permalink":"/event/umdbb/","publishdate":"2022-01-06T00:00:00Z","relpermalink":"/event/umdbb/","section":"event","summary":"ICPR workshop on bias and fairness of biometric systems.","tags":[],"title":"Understanding and Mitigating Demographic Bias in Biometric Systems(UMDBB)","type":"event"},{"authors":null,"categories":[],"content":"Introduction Fairness evaluation studies across Biometric Modalities Fairness in Face Recognition Ocular : Motivation Research Objectives Covariates to be Studied Datasets Used Gender Classification Across Models Across Models on UFPR Periocular - RGB Across Models on Notredame Ocular - NIR Across Varying Data Balances and Gender UFPR - RGB Notredame - NIR Across Age UFPR - RGB TABLE I: Accuracy of CNN-based Gender Classification on Left Ocular Region among Young (18 to 39 years), Middle (40\rto 59 years) and Older Adults (60 to 79 years).\r\r\r\rCNN\rYoung\rMiddle-Aged\rOlder\r\r\r\r\r\rMale[%]\rFemale[%]\rMale[%]\rFemale[%]\rMale[%]\rFemale[%]\r\r\rResNet-50\r98.39\r98.19\r100\r96.67\r99.17\r98.06\r\r\rMobileNet-V2\r99.97\r99.9\r100\r99.7\r100\r100\r\r\rShuffleNet-V2-50\r98.23\r97.57\r98.28\r97.56\r94.76\r98.89\r\r\rEfficientNet-B0\r95.89\r97.54\r98.58\r96.44\r95.23\r86.94\r\r\r\r Middle Aged adults slightly outperformed the other two groups in gender classification by about 1% − 2%. Possible explanation: Stable and distinct gender cues for middle aged adults when compared to young and older adults. Younger adult population performed the best in age classification by about 25%. This could be due to distinct variation in the features attributed to the growing stage of the youth population over middle-aged and older adults  Across Race VISOB + Notredame- Ocular RGB TABLE II: Gender Classification results across race groups - Ocular Datasets.\r\r\rRace\rMale\rFemale\r\r\r\r\rWhite\r82.42\r89.92\r\r\rSouth Asian\r96.51\r88.88\r\rEast Asian\r26.50\r72.72\r --\r\rBlack\r78.57\r72.10\r\r\rMiddle Eastern\r71.12\r93.08\r\r\rLatino\r84.83\r91.61\r\r\r\r South Asians and Caucasians overall works better. This make sense since it contains the majority of the dataset  Face VS Ocular FlickrFaceHQ-Aging - RGB  Face performs better than Ocular in Gender Classification on RGB This may be because of the low quality of the crop  TABLE III: Gender Classification results across Face vs Periocular on FFHQ-Aging.\r\r\rMale\rFemale\rModality\r\r\r\r\r97.6\r96.8\rFace\r\r\r92.5\r91.6\rOcular\r\r\r\rNotredame - NIR Face Ocular  Face performs better than Ocular in Gender Classification on NIR This may be because of the low quality of the crop  Subject Verification Across Gender UFPR - RGB Training Set : All Training Set : Male Training Set : Female Notredame - NIR Training Set : All Training Set : Male Training Set : Female VISOB - RGB Across Varying Data Balances and Models UFPR - RGB Notredame - NIR Across Age UFPR - RGB TABLE I: EER and FNMR at 0.01 and 0.1 FMR for user authentication using CNN models for Left (L), Right (R) ocular region,and their score-level fusion (L+R) for Young, Middle-Aged and Older adults evaluated on balanced version of UFPR ocular datasets.\r\r\rCNN\n\rAge-Group\n\rEER(%)\u0026nbsp;\u0026nbsp;\rFNMR(%) @ FMR\r\r\r0.01\r0.1\r\r\rL\rR\rL+R\rL\rR\rL+R\rL\rR\rL+R\r\r\r\r\rResNet-50\rYoung\r8.60\r9.52\r9.06\r37.63\r35.35\r36.49\r53.74\r54.03\r53.89\r\r\rMiddle-Aged\r8.62\r9.08\r8.85\r30.76\r25.04\r27.9\r52.23\r48.55\r50.39\r\r\rOlder\r11.01\r11.00\r11.01\r15.47\r19.34\r17.405\r30.67\r30.67\r30.67\r\r\rMobileNet-V2\rYoung\r7.75\r7.32\r7.54\r33.21\r26.11\r29.66\r51.61\r48.28\r49.95\r\r\rMiddle-Aged\r9.08\r8.68\r8.88\r29.18\r28.14\r28.66\r51.17\r51.31\r51.24\r\r\rOlder\r8.04\r9.63\r8.84\r18.54\r13.47\r16.005\r39.47\r36.00\r37.74\r\r\rShuffleNet-V2\rYoung\r6.93\r6.96\r6.95\r37.63\r38.09\r37.86\r56.44\r56.26\r56.35\r\r\rMiddle-Aged\r8.32\r9.25\r8.79\r37.38\r46.07\r41.73\r55.08\r61.10\r58.09\r\r\rOlder\r9.72\r8.18\r8.95\r30.53\r30.80\r30.67\r44.93\r53.47\r49.20\r\r\rEfficientNet-B0\rYoung\r7.64\r9.61\r8.63\r32.54\r27.00\r29.77\r55.40\r48.38\r51.89\r\r\rMiddle-Aged\r6.95\r9.03\r7.99\r38.07\r26.69\r32.38\r49.12\r49.31\r49.22\r\r\rOlder\r9.34\r12.08\r10.71\r20.80\r23.33\r22.065\r39.73\r41.74\r40.74\r\r\r\r Younger adults obtained performance identical to middle-aged individuals in user verification. Older adults’ performance differs slightly in terms of EER with only a 1% decrease, but the performance dropped at lower FMR points. The possible reason could be due to likely inferior quality of image capture, and relatively higher inter-class similarity due to wrinkles and folds on the skin.  ","date":1656693352,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656693352,"objectID":"b4379fcc902cdb6ae442df033c091ea4","permalink":"/updates/report/","publishdate":"2022-07-01T11:35:52-05:00","relpermalink":"/updates/report/","section":"updates","summary":"Introduction Fairness evaluation studies across Biometric Modalities Fairness in Face Recognition Ocular : Motivation Research Objectives Covariates to be Studied Datasets Used Gender Classification Across Models Across Models on UFPR Periocular - RGB Across Models on Notredame Ocular - NIR Across Varying Data Balances and Gender UFPR - RGB Notredame - NIR Across Age UFPR - RGB TABLE I: Accuracy of CNN-based Gender Classification on Left Ocular Region among Young (18 to 39 years), Middle (40\rto 59 years) and Older Adults (60 to 79 years).","tags":[],"title":"Report","type":"updates"},{"authors":null,"categories":[],"content":"Gender Classification Across Race on Ocular  Training and Testing Dataset used - VISOB + Notredame Test Data is Limited and Imbalanced  TABLE I: Gender Classification results across race groups - Ocular Datasets.\r\r\rRace\rMale\rFemale\r\r\r\r\rWhite\r82.42\r89.92\r\r\rSouth Asian\r96.51\r88.88\r\rEast Asian\r26.50\r72.72\r --\r\rBlack\r78.57\r72.10\r\r\rMiddle Eastern\r71.12\r93.08\r\r\rLatino\r84.83\r91.61\r\r\r\rFace vs PeriOcular Modality (RGB Spectrum)  Dataset Under Investigation - FlickrFaceHQ Aging Resolution - 1024 X 1024  TABLE II: Gender Classification results across Face vs Periocular on FFHQ-Aging.\r\r\rMale\rFemale\rModality\r\r\r\r\r97.6\r96.8\rFace\r\r\r92.5\r91.6\rOcular\r\r\r\r---","date":1651509352,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651509352,"objectID":"b4aeff460cb3ff6a521690944dfb7247","permalink":"/updates/misc-results/","publishdate":"2022-05-02T11:35:52-05:00","relpermalink":"/updates/misc-results/","section":"updates","summary":"Gender Classification Across Race on Ocular  Training and Testing Dataset used - VISOB + Notredame Test Data is Limited and Imbalanced  TABLE I: Gender Classification results across race groups - Ocular Datasets.","tags":[],"title":"Miscellaneous Results","type":"updates"},{"authors":null,"categories":[],"content":"Dataset Gender Classification Results              Subject Verification Results        Subject Verification All Results Table  Visualizations GradCAM GradCAM         Guided GradCAM Guided GradCAM         Occlusion Sensitivity Occlusion Sensitivity         Vanilla Gradients Vanilla Gradients         Integrated Gradients Integrated Gradients         SmoothGrad         -- Gradients x Input Gradients x Input         Averaged Map \u0026times;       References Bibliography called, but no references ","date":1648312552,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648312552,"objectID":"a13db798a5b55a04223e32f31d7512c3","permalink":"/updates/notredame-results/","publishdate":"2022-03-26T11:35:52-05:00","relpermalink":"/updates/notredame-results/","section":"updates","summary":"Dataset Gender Classification Results              Subject Verification Results        Subject Verification All Results Table  Visualizations GradCAM GradCAM         Guided GradCAM Guided GradCAM         Occlusion Sensitivity Occlusion Sensitivity         Vanilla Gradients Vanilla Gradients         Integrated Gradients Integrated Gradients         SmoothGrad         -- Gradients x Input Gradients x Input         Averaged Map \u0026times;       References Bibliography called, but no references ","tags":[],"title":"Notredame Preliminary Results","type":"updates"},{"authors":null,"categories":[],"content":"Dataset The UFPR-Periocular dataset was created to obtain images in unconstrained scenarios that contain realistic noises caused by occlusion, blur, and variations in lighting, distance, and angles.\nThe gender distribution of the subjects is $(53,65\\%)$ male and $(46,35\\%)$ female, and approximately $66\\%$ of the subjects are under $31$ years old. In total, the dataset has images captured from $196$ different mobile devices – the five most used device models were: Apple iPhone 8 $(4.1\\%)$, Apple iPhone 9 $(3.1\\%)$, Xiaomi Mi 8 Lite $(3.0\\%)$, Apple iPhone 7 $(3.0\\%)$, and Samsung Galaxy J7 Prime $(2.7\\%)$.\nGender Classification Results              Subject Verification Results        Subject Verification All Results Table  Visualizations GradCAM GradCAM         Guided GradCAM Guided GradCAM         Occlusion Sensitivity Occlusion Sensitivity         Vanilla Gradients Vanilla Gradients         Integrated Gradients Integrated Gradients         SmoothGrad SmoothGrad         Gradients x Input Gradients x Input         Averaged Map \u0026times;       References Bibliography called, but no references ","date":1645547752,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645547752,"objectID":"c155b7f4ec734e028eaee7a3857a0979","permalink":"/updates/ufpr-results/","publishdate":"2022-02-22T11:35:52-05:00","relpermalink":"/updates/ufpr-results/","section":"updates","summary":"Dataset The UFPR-Periocular dataset was created to obtain images in unconstrained scenarios that contain realistic noises caused by occlusion, blur, and variations in lighting, distance, and angles.\nThe gender distribution of the subjects is $(53,65\\%)$ male and $(46,35\\%)$ female, and approximately $66\\%$ of the subjects are under $31$ years old.","tags":[],"title":"UFPR Preliminary Results","type":"updates"},{"authors":null,"categories":[],"content":"Dataset   Dataset includes 33,660 ocular samples from 1,122 subjects captured by 196 different mobile.\n  All images were resized to224×224. the complete dataset is classified into three age-groups: namely young (18 to 39), middle-aged (40 to 59), and older adults (60 to 79).\n  User recognition and gender classifier models are fine-tuned on randomly selected gender and age-group balanced subset from 780 participants. Subject-disjoint, gender and age-group-balanced subset selected from 342 subjects are used as the test set for authentication and 260 subjects (130 males and females) for gender classification.\n  Age classifier models are fine-tuned on balanced subset selected from 432 subjects and evaluated on subset from 132 subjects, across 6 age-groups, namely 18−29, 30−39, 40−49, 50−59, 60−69 and 70−79.\n  TABLE I: EER and FNMR at 0.01 and 0.1 FMR for user authentication using CNN models for Left (L), Right (R) ocular region,and their score-level fusion (L+R) for Young, Middle-Aged and Older adults evaluated on balanced version of UFPR ocular datasets.   CNN\n Age-Group\n EER(%)\u0026nbsp;\u0026nbsp; FNMR(%) @ FMR   0.01 0.1   L R L+R L R L+R L R L+R     ResNet-50 Young 8.60 9.52 9.06 37.63 35.35 36.49 53.74 54.03 53.89   Middle-Aged 8.62 9.08 8.85 30.76 25.04 27.9 52.23 48.55 50.39   Older 11.01 11.00 11.01 15.47 19.34 17.405 30.67 30.67 30.67   MobileNet-V2 Young 7.75 7.32 7.54 33.21 26.11 29.66 51.61 48.28 49.95   Middle-Aged 9.08 8.68 8.88 29.18 28.14 28.66 51.17 51.31 51.24   Older 8.04 9.63 8.84 18.54 13.47 16.005 39.47 36.00 37.74   ShuffleNet-V2 Young 6.93 6.96 6.95 37.63 38.09 37.86 56.44 56.26 56.35   Middle-Aged 8.32 9.25 8.79 37.38 46.07 41.73 55.08 61.10 58.09   Older 9.72 8.18 8.95 30.53 30.80 30.67 44.93 53.47 49.20   EfficientNet-B0 Young 7.64 9.61 8.63 32.54 27.00 29.77 55.40 48.38 51.89   Middle-Aged 6.95 9.03 7.99 38.07 26.69 32.38 49.12 49.31 49.22   Older 9.34 12.08 10.71 20.80 23.33 22.065 39.73 41.74 40.74    TABLE II: Accuracy of CNN-based Gender Classification on Left Ocular Region among Young (18 to 39 years), Middle (40 to 59 years) and Older Adults (60 to 79 years).    CNN Young Middle-Aged Older      Male[%] Female[%] Male[%] Female[%] Male[%] Female[%]   ResNet-50 98.39 98.19 100 96.67 99.17 98.06   MobileNet-V2 99.97 99.9 100 99.7 100 100   ShuffleNet-V2-50 98.23 97.57 98.28 97.56 94.76 98.89   EfficientNet-B0 95.89 97.54 98.58 96.44 95.23 86.94    TABLE III: Accuracy of the CNN-based Gender Classification on Right Ocular Region among Young (18 to 39 years), Middle (40 to 59 years) and Older Adults (60 to 79 years).   CNN Young Middle-Aged Older      Male[%] Female[%] Male[%] Female[%] Male[%] Female[%]   ResNet-50 97.98 99.61 97.07 99.78 92.86 98.61   MobileNet-V2 96.84 92.35 95.86 98.66 94.76 97.5   ShuffleNet-V2-50 98.51 98.91 98.79 99.33 97.38 98.61   EfficientNet-B0 97.18 95.14 95.15 97.33 94.52 90    TABLE IV: Exact and 1-off accuracies of Age-group Classification for Young Adults    Left Ocular Right Ocular     CNN Exact [%]  1-off [%] Exact [%]  1-off [%]   ResNet-50 46.72 93 52.87 93.16   MobileNet-V2 54.14 91.78 59.76 94.9   ShuffleNet-V2-50 52.47 91.16 45.16 85.27   EfficientNet-B0 28.225 53.195 31.6 62.64    TABLE V: Exact and 1-off accuracies of Age-group Classification for Middle-Aged Adults.    Left Ocular Right Ocular     CNN Exact [%]  1-off [%] Exact [%]  1-off [%]   ResNet-50 31.61 81.73 35.03 78.43   MobileNet-V2 27.95 86.35 31.86 72.69   ShuffleNet-V2-50 28.46 75.2 24.61 47.98   EfficientNet-B0 20.93 57.86 19.56 55.73    TABLE VI: Exact and 1-off accuracies of Age-group Classification for Older Adults.    Left Ocular Right Ocular     CNN Exact [%]  1-off [%] Exact [%]  1-off [%]   ResNet-50 28 82.3 29.485 71.43   MobileNet-V2 38.07 79.82 32.93 76.65   ShuffleNet-V2-50 36.04 89.65 18.54 56.85   EfficientNet-B0 4.97 32.3 4.74 27    References Bibliography called, but no references ","date":1642005352,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642005352,"objectID":"2d5d1b8e3989af34774c16879a04c0e2","permalink":"/updates/ufpr-age/","publishdate":"2022-01-12T11:35:52-05:00","relpermalink":"/updates/ufpr-age/","section":"updates","summary":"Dataset   Dataset includes 33,660 ocular samples from 1,122 subjects captured by 196 different mobile.\n  All images were resized to224×224. the complete dataset is classified into three age-groups: namely young (18 to 39), middle-aged (40 to 59), and older adults (60 to 79).","tags":[],"title":"UFPR Fairness - Across Age","type":"updates"},{"authors":["Anoop Krishnan","Ali Almadan","Ajita Rattani"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1634083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634083200,"objectID":"99f1735efaaff9ce390130a4b2a0e8bf","permalink":"/publication/iccst_2021/","publishdate":"2021-10-13T00:00:00Z","relpermalink":"/publication/iccst_2021/","section":"publication","summary":"A number of studies suggest bias of the face biometrics, i.e., face recognition and soft-biometric estimation methods, across gender, race, and age-groups. There is a recent urge to investigate the bias of different biometric modalities toward the deployment of fair and trustworthy biometric solutions. Ocular biometrics has obtained increased attention from academia and industry due to its high accuracy, security, privacy, and ease of use in mobile devices. A recent study in $2020$ also suggested the fairness of ocular-based user recognition across males and females. This paper aims to evaluate the fairness of ocular biometrics in the visible spectrum among age-groups; young, middle, and older adults. Thanks to the availability of the latest large-scale $2020$ UFPR ocular biometric dataset, with subjects acquired in the age range $18$ - $79$ years, to facilitate this study. Experimental results suggest the overall equivalent performance of ocular biometrics across gender and age-groups in user verification and gender-classification. Performance difference for older adults at lower false match rate and young adults was noted at user verification and age-classification, respectively. This could be attributed to inherent characteristics of the biometric data from these age-groups impacting specific applications, which suggest a need for advancement in sensor technology and software solutions.","tags":["Fairness","Bias","Ocular","Analysis"],"title":"Investigating Fairness of Ocular Biometrics Among Young, Middle-Aged, and Older Adults","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]